{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap-southeast-1\n"
     ]
    }
   ],
   "source": [
    "# Save anime_train.recordio and anime_test.recordio to S3\n",
    "# Set the output path for the saved model\n",
    "bucket_name = 'sagemaker-tutorial-rnd'\n",
    "prefix = \"Recommendation\"\n",
    "\n",
    "model_output = f\"s3://{bucket_name}/{prefix}/saved_model\"\n",
    "train_input = sagemaker.TrainingInput(\n",
    "    f\"s3://{bucket_name}/{prefix}/train/anime_train.recordio\")\n",
    "test_input = sagemaker.TrainingInput(\n",
    "    f\"s3://{bucket_name}/{prefix}/test/anime_test.recordio\")\n",
    "\n",
    "print(sagemaker.Session().boto_region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dimension = (Number of distinct user id: 69) + (Number of distinct anime id: 3098)\n",
    "dim = 69 + 3098\n",
    "\n",
    "# Build estimator\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    \"factorization-machines\", sagemaker.Session().boto_region_name)\n",
    "\n",
    "base_job_name = \"recommentation-anime\"\n",
    "\n",
    "recommendation_estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role=get_execution_role(),\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=model_output,\n",
    "    use_spot_instances=False,\n",
    "    max_run=3600,\n",
    "    # max_wait=3600,\n",
    "    # checkpoint_s3_uri=f\"s3://{bucket_name}/{prefix}/checkpoints/{base_job_name}\",\n",
    "    base_job_name=base_job_name,\n",
    ")\n",
    "\n",
    "# Set the hyperparameter\n",
    "recommendation_estimator.set_hyperparameters(\n",
    "    feature_dim=dim,\n",
    "    num_factors=8,\n",
    "    predictor_type='regressor',\n",
    "    mini_batch_size=2000,\n",
    "    epochs=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-12 11:13:50 Starting - Starting the training job...\n",
      "2022-02-12 11:14:14 Starting - Preparing the instances for trainingProfilerReport-1644664430: InProgress\n",
      "......\n",
      "2022-02-12 11:15:18 Downloading - Downloading input data\n",
      "2022-02-12 11:15:18 Training - Downloading the training image......\n",
      "2022-02-12 11:16:14 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:87: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:120: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840 integration.py:636] worker started\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-conf.json: {'epochs': 1, 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0'}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'epochs': '100', 'feature_dim': '3167', 'mini_batch_size': '2000', 'num_factors': '8', 'predictor_type': 'regressor'}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] Final configuration: {'epochs': '100', 'mini_batch_size': '2000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0', 'feature_dim': '3167', 'num_factors': '8', 'predictor_type': 'regressor'}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 WARNING 140146954667840] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] Using default worker.\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:18.308] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:18.317] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 15, \"num_examples\": 1, \"num_bytes\": 127800}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] nvidia-smi: took 0.029 seconds to run.\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.301809, \"EndTime\": 1644664578.352314, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 37.50872611999512, \"count\": 1, \"min\": 37.50872611999512, \"max\": 37.50872611999512}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.352438, \"EndTime\": 1644664578.3524806, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2000.0, \"count\": 1, \"min\": 2000, \"max\": 2000}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 2000.0, \"count\": 1, \"min\": 2000, \"max\": 2000}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[34m[11:16:18] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.205844.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[11:16:18] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.205844.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=0, batch=0 train rmse <loss>=7.6097331794551115\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=0, batch=0 train mse <loss>=57.9080390625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=0, batch=0 train absolute_loss <loss>=7.39315185546875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:18.454] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 88, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=0, train rmse <loss>=7.693843229024913\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=0, train mse <loss>=59.1952236328125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=0, train absolute_loss <loss>=7.513759033203125\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.3523872, \"EndTime\": 1644664578.455711, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"update.time\": {\"sum\": 102.9660701751709, \"count\": 1, \"min\": 102.9660701751709, \"max\": 102.9660701751709}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.352701, \"EndTime\": 1644664578.4562232, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 10000.0, \"count\": 1, \"min\": 10000, \"max\": 10000}, \"Total Batches Seen\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=77077.44398553754 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=1, batch=0 train rmse <loss>=7.218274605558451\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=1, batch=0 train mse <loss>=52.10348828125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=1, batch=0 train absolute_loss <loss>=6.989658203125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:18.507] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 49, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=1, train rmse <loss>=7.301210435819871\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=1, train mse <loss>=53.307673828125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=1, train absolute_loss <loss>=7.111255737304687\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.4559455, \"EndTime\": 1644664578.5077338, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 50.96721649169922, \"count\": 1, \"min\": 50.96721649169922, \"max\": 50.96721649169922}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.4567385, \"EndTime\": 1644664578.5079708, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 18000.0, \"count\": 1, \"min\": 18000, \"max\": 18000}, \"Total Batches Seen\": {\"sum\": 9.0, \"count\": 1, \"min\": 9, \"max\": 9}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=155723.8089050187 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=2, batch=0 train rmse <loss>=6.8305166006130165\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=2, batch=0 train mse <loss>=46.65595703125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=2, batch=0 train absolute_loss <loss>=6.5885556640625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:18.547] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 37, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=2, train rmse <loss>=6.912226345170924\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=2, train mse <loss>=47.778873046875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=2, train absolute_loss <loss>=6.711454345703125\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.5078065, \"EndTime\": 1644664578.5480292, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 39.6270751953125, \"count\": 1, \"min\": 39.6270751953125, \"max\": 39.6270751953125}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.5083735, \"EndTime\": 1644664578.5482743, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 26000.0, \"count\": 1, \"min\": 26000, \"max\": 26000}, \"Total Batches Seen\": {\"sum\": 13.0, \"count\": 1, \"min\": 13, \"max\": 13}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=199816.77643724022 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=3, batch=0 train rmse <loss>=6.447767600592472\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=3, batch=0 train mse <loss>=41.57370703125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=3, batch=0 train absolute_loss <loss>=6.19517333984375\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:18.594] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 44, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=3, train rmse <loss>=6.528539808506187\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=3, train mse <loss>=42.62183203125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=3, train absolute_loss <loss>=6.318119506835938\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.5481029, \"EndTime\": 1644664578.5947537, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 46.170711517333984, \"count\": 1, \"min\": 46.170711517333984, \"max\": 46.170711517333984}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.5485554, \"EndTime\": 1644664578.5953407, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 34000.0, \"count\": 1, \"min\": 34000, \"max\": 34000}, \"Total Batches Seen\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=169732.57119732915 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=4, batch=0 train rmse <loss>=6.071879664177313\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=4, batch=0 train mse <loss>=36.86772265625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=4, batch=0 train absolute_loss <loss>=5.810587890625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:18.657] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 60, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=4, train rmse <loss>=6.151973451157766\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=4, train mse <loss>=37.84677734375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=4, train absolute_loss <loss>=5.9315205078125\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.5950208, \"EndTime\": 1644664578.659053, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 63.05265426635742, \"count\": 1, \"min\": 63.05265426635742, \"max\": 63.05265426635742}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.5959759, \"EndTime\": 1644664578.6592848, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 42000.0, \"count\": 1, \"min\": 42000, \"max\": 42000}, \"Total Batches Seen\": {\"sum\": 21.0, \"count\": 1, \"min\": 21, \"max\": 21}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=126024.61568508149 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=5, batch=0 train rmse <loss>=5.704714333338699\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=5, batch=0 train mse <loss>=32.543765625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=5, batch=0 train absolute_loss <loss>=5.43270947265625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:18.709] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 44, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=5, train rmse <loss>=5.784316163261004\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=5, train mse <loss>=33.4583134765625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=5, train absolute_loss <loss>=5.55327783203125\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.6591141, \"EndTime\": 1644664578.710222, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 50.61817169189453, \"count\": 1, \"min\": 50.61817169189453, \"max\": 50.61817169189453}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.6595764, \"EndTime\": 1644664578.710439, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 50000.0, \"count\": 1, \"min\": 50000, \"max\": 50000}, \"Total Batches Seen\": {\"sum\": 25.0, \"count\": 1, \"min\": 25, \"max\": 25}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 7.0, \"count\": 1, \"min\": 7, \"max\": 7}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=156896.12089926307 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=6, batch=0 train rmse <loss>=5.348073360741698\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=6, batch=0 train mse <loss>=28.601888671875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=6, batch=0 train absolute_loss <loss>=5.06776025390625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:18.776] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 64, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=6, train rmse <loss>=5.427271224954466\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=6, train mse <loss>=29.45527294921875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=6, train absolute_loss <loss>=5.186062255859375\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.7102861, \"EndTime\": 1644664578.7772791, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 66.56551361083984, \"count\": 1, \"min\": 66.56551361083984, \"max\": 66.56551361083984}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.7106867, \"EndTime\": 1644664578.7775195, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 58000.0, \"count\": 1, \"min\": 58000, \"max\": 58000}, \"Total Batches Seen\": {\"sum\": 29.0, \"count\": 1, \"min\": 29, \"max\": 29}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=119462.23106747033 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=7, batch=0 train rmse <loss>=5.003660769251849\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=7, batch=0 train mse <loss>=25.03662109375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=7, batch=0 train absolute_loss <loss>=4.71256982421875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:18.822] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 42, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=7, train rmse <loss>=5.082423901530031\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=7, train mse <loss>=25.83103271484375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=7, train absolute_loss <loss>=4.829123291015625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.7773566, \"EndTime\": 1644664578.8233871, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 45.562744140625, \"count\": 1, \"min\": 45.562744140625, \"max\": 45.562744140625}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.7777963, \"EndTime\": 1644664578.823844, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 66000.0, \"count\": 1, \"min\": 66000, \"max\": 66000}, \"Total Batches Seen\": {\"sum\": 33.0, \"count\": 1, \"min\": 33, \"max\": 33}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 9.0, \"count\": 1, \"min\": 9, \"max\": 9}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=173151.99240399204 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=8, batch=0 train rmse <loss>=4.673056706736502\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=8, batch=0 train mse <loss>=21.837458984375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=8, batch=0 train absolute_loss <loss>=4.37114892578125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:18.880] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 54, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=8, train rmse <loss>=4.751220803235075\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=8, train mse <loss>=22.57409912109375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=8, train absolute_loss <loss>=4.488365112304687\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.8234868, \"EndTime\": 1644664578.8808055, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 56.63013458251953, \"count\": 1, \"min\": 56.63013458251953, \"max\": 56.63013458251953}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.8241444, \"EndTime\": 1644664578.8810632, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 74000.0, \"count\": 1, \"min\": 74000, \"max\": 74000}, \"Total Batches Seen\": {\"sum\": 37.0, \"count\": 1, \"min\": 37, \"max\": 37}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=140195.67142976518 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=9, batch=0 train rmse <loss>=4.357704204911114\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=9, batch=0 train mse <loss>=18.9895859375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=9, batch=0 train absolute_loss <loss>=4.051035400390625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:18.928] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 44, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=9, train rmse <loss>=4.434959054648222\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=9, train mse <loss>=19.66886181640625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=9, train absolute_loss <loss>=4.163435607910157\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.8808815, \"EndTime\": 1644664578.9285295, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 47.15466499328613, \"count\": 1, \"min\": 47.15466499328613, \"max\": 47.15466499328613}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.8813481, \"EndTime\": 1644664578.9287682, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 82000.0, \"count\": 1, \"min\": 82000, \"max\": 82000}, \"Total Batches Seen\": {\"sum\": 41.0, \"count\": 1, \"min\": 41, \"max\": 41}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 11.0, \"count\": 1, \"min\": 11, \"max\": 11}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=168227.21461553502 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=10, batch=0 train rmse <loss>=4.058900328290902\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=10, batch=0 train mse <loss>=16.474671875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=10, batch=0 train absolute_loss <loss>=3.743304931640625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:18.976] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 45, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=10, train rmse <loss>=4.13478216886747\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=10, train mse <loss>=17.096423583984375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #quality_metric: host=algo-1, epoch=10, train absolute_loss <loss>=3.851579528808594\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.9286027, \"EndTime\": 1644664578.9772947, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 48.18868637084961, \"count\": 1, \"min\": 48.18868637084961, \"max\": 48.18868637084961}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.9290767, \"EndTime\": 1644664578.9780393, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 90000.0, \"count\": 1, \"min\": 90000, \"max\": 90000}, \"Total Batches Seen\": {\"sum\": 45.0, \"count\": 1, \"min\": 45, \"max\": 45}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:18 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=162151.9726286896 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=11, batch=0 train rmse <loss>=3.7777932288159315\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=11, batch=0 train mse <loss>=14.2717216796875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=11, batch=0 train absolute_loss <loss>=3.452738037109375\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:19.039] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 59, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=11, train rmse <loss>=3.85167833877527\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=11, train mse <loss>=14.835426025390625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=11, train absolute_loss <loss>=3.56034033203125\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.97754, \"EndTime\": 1644664579.0409634, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 62.25156784057617, \"count\": 1, \"min\": 62.25156784057617, \"max\": 62.25156784057617}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664578.9786577, \"EndTime\": 1644664579.0415857, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 98000.0, \"count\": 1, \"min\": 98000, \"max\": 98000}, \"Total Batches Seen\": {\"sum\": 49.0, \"count\": 1, \"min\": 49, \"max\": 49}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 13.0, \"count\": 1, \"min\": 13, \"max\": 13}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=126539.32194441302 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=12, batch=0 train rmse <loss>=3.515375685604243\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=12, batch=0 train mse <loss>=12.3578662109375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=12, batch=0 train absolute_loss <loss>=3.1902470703125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:19.092] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 48, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=12, train rmse <loss>=3.586479023653892\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=12, train mse <loss>=12.862831787109375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=12, train absolute_loss <loss>=3.2888897705078124\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.0412588, \"EndTime\": 1644664579.0933669, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 51.199913024902344, \"count\": 1, \"min\": 51.199913024902344, \"max\": 51.199913024902344}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.0421383, \"EndTime\": 1644664579.093962, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 106000.0, \"count\": 1, \"min\": 106000, \"max\": 106000}, \"Total Batches Seen\": {\"sum\": 53.0, \"count\": 1, \"min\": 53, \"max\": 53}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=153222.18163220576 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=13, batch=0 train rmse <loss>=3.272475768626866\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=13, batch=0 train mse <loss>=10.70909765625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=13, batch=0 train absolute_loss <loss>=2.9410322265625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:19.140] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 28, \"duration\": 43, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=13, train rmse <loss>=3.3398548793674214\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=13, train mse <loss>=11.154630615234375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=13, train absolute_loss <loss>=3.031570007324219\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.0936348, \"EndTime\": 1644664579.1414032, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 46.73504829406738, \"count\": 1, \"min\": 46.73504829406738, \"max\": 46.73504829406738}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.0946276, \"EndTime\": 1644664579.141869, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 114000.0, \"count\": 1, \"min\": 114000, \"max\": 114000}, \"Total Batches Seen\": {\"sum\": 57.0, \"count\": 1, \"min\": 57, \"max\": 57}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 15.0, \"count\": 1, \"min\": 15, \"max\": 15}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=168377.47702992256 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=14, batch=0 train rmse <loss>=3.0497404794814096\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=14, batch=0 train mse <loss>=9.3009169921875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=14, batch=0 train absolute_loss <loss>=2.705400390625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:19.208] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 30, \"duration\": 64, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=14, train rmse <loss>=3.1123093291560147\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=14, train mse <loss>=9.686469360351563\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=14, train absolute_loss <loss>=2.79382666015625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.1416097, \"EndTime\": 1644664579.208711, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 66.33472442626953, \"count\": 1, \"min\": 66.33472442626953, \"max\": 66.33472442626953}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.1423483, \"EndTime\": 1644664579.2089286, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 122000.0, \"count\": 1, \"min\": 122000, \"max\": 122000}, \"Total Batches Seen\": {\"sum\": 61.0, \"count\": 1, \"min\": 61, \"max\": 61}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=119926.3452850689 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=15, batch=0 train rmse <loss>=2.8476143901244337\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=15, batch=0 train mse <loss>=8.10890771484375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=15, batch=0 train absolute_loss <loss>=2.50884765625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:19.263] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 52, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=15, train rmse <loss>=2.9041651628254117\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=15, train mse <loss>=8.43417529296875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=15, train absolute_loss <loss>=2.5901112060546874\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.2087793, \"EndTime\": 1644664579.2643213, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 55.145978927612305, \"count\": 1, \"min\": 55.145978927612305, \"max\": 55.145978927612305}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.2091503, \"EndTime\": 1644664579.2647853, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 130000.0, \"count\": 1, \"min\": 130000, \"max\": 130000}, \"Total Batches Seen\": {\"sum\": 65.0, \"count\": 1, \"min\": 65, \"max\": 65}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=143144.810992752 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=16, batch=0 train rmse <loss>=2.666306789274492\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=16, batch=0 train mse <loss>=7.10919189453125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=16, batch=0 train absolute_loss <loss>=2.3335029296875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:19.316] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 34, \"duration\": 50, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=16, train rmse <loss>=2.7155481999712667\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=16, train mse <loss>=7.374202026367188\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=16, train absolute_loss <loss>=2.4017777099609376\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.2645435, \"EndTime\": 1644664579.3170907, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 51.82337760925293, \"count\": 1, \"min\": 51.82337760925293, \"max\": 51.82337760925293}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.2652442, \"EndTime\": 1644664579.3172498, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 138000.0, \"count\": 1, \"min\": 138000, \"max\": 138000}, \"Total Batches Seen\": {\"sum\": 69.0, \"count\": 1, \"min\": 69, \"max\": 69}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=153510.9891115381 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=17, batch=0 train rmse <loss>=2.505757432893296\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=17, batch=0 train mse <loss>=6.2788203125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=17, batch=0 train absolute_loss <loss>=2.16979638671875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:19.373] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 36, \"duration\": 54, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=17, train rmse <loss>=2.5463666095352906\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=17, train mse <loss>=6.48398291015625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=17, train absolute_loss <loss>=2.2262353515625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.3171449, \"EndTime\": 1644664579.3746362, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 57.19184875488281, \"count\": 1, \"min\": 57.19184875488281, \"max\": 57.19184875488281}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.3174195, \"EndTime\": 1644664579.3752756, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 146000.0, \"count\": 1, \"min\": 146000, \"max\": 146000}, \"Total Batches Seen\": {\"sum\": 73.0, \"count\": 1, \"min\": 73, \"max\": 73}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=137379.65821344056 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=18, batch=0 train rmse <loss>=2.365600127841506\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=18, batch=0 train mse <loss>=5.59606396484375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=18, batch=0 train absolute_loss <loss>=2.017619140625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:19.427] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 50, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=18, train rmse <loss>=2.3962905191051282\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=18, train mse <loss>=5.742208251953125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=18, train absolute_loss <loss>=2.063941467285156\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.374931, \"EndTime\": 1644664579.4290333, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 53.030967712402344, \"count\": 1, \"min\": 53.030967712402344, \"max\": 53.030967712402344}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.3759751, \"EndTime\": 1644664579.4297795, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 154000.0, \"count\": 1, \"min\": 154000, \"max\": 154000}, \"Total Batches Seen\": {\"sum\": 77.0, \"count\": 1, \"min\": 77, \"max\": 77}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=147456.36000087892 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=19, batch=0 train rmse <loss>=2.2451409120239534\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=19, batch=0 train mse <loss>=5.04065771484375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=19, batch=0 train absolute_loss <loss>=1.88755322265625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:19.500] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 40, \"duration\": 68, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=19, train rmse <loss>=2.2647380268097512\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=19, train mse <loss>=5.129038330078125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=19, train absolute_loss <loss>=1.9363978271484374\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.4293728, \"EndTime\": 1644664579.5016294, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 70.9531307220459, \"count\": 1, \"min\": 70.9531307220459, \"max\": 70.9531307220459}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.430627, \"EndTime\": 1644664579.5022464, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 162000.0, \"count\": 1, \"min\": 162000, \"max\": 162000}, \"Total Batches Seen\": {\"sum\": 81.0, \"count\": 1, \"min\": 81, \"max\": 81}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 21.0, \"count\": 1, \"min\": 21, \"max\": 21}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=111107.02282442774 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=20, batch=0 train rmse <loss>=2.1433503408434538\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=20, batch=0 train mse <loss>=4.59395068359375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=20, batch=0 train absolute_loss <loss>=1.79676025390625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:19.555] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 42, \"duration\": 51, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=20, train rmse <loss>=2.150869773763955\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=20, train mse <loss>=4.626240783691406\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=20, train absolute_loss <loss>=1.8320469970703126\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.5019002, \"EndTime\": 1644664579.5566661, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 53.70378494262695, \"count\": 1, \"min\": 53.70378494262695, \"max\": 53.70378494262695}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.502917, \"EndTime\": 1644664579.5571878, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 20, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 170000.0, \"count\": 1, \"min\": 170000, \"max\": 170000}, \"Total Batches Seen\": {\"sum\": 85.0, \"count\": 1, \"min\": 85, \"max\": 85}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=146516.94656221892 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=21, batch=0 train rmse <loss>=2.0588861568895083\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=21, batch=0 train mse <loss>=4.23901220703125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=21, batch=0 train absolute_loss <loss>=1.713491943359375\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:19.610] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 51, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=21, train rmse <loss>=2.0535999230724005\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=21, train mse <loss>=4.217272644042969\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=21, train absolute_loss <loss>=1.7365101318359375\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.5569081, \"EndTime\": 1644664579.6112561, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 53.46941947937012, \"count\": 1, \"min\": 53.46941947937012, \"max\": 53.46941947937012}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.5577612, \"EndTime\": 1644664579.6114717, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 21, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 178000.0, \"count\": 1, \"min\": 178000, \"max\": 178000}, \"Total Batches Seen\": {\"sum\": 89.0, \"count\": 1, \"min\": 89, \"max\": 89}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 23.0, \"count\": 1, \"min\": 23, \"max\": 23}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=148623.50731724603 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=22, batch=0 train rmse <loss>=1.9901407377997165\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=22, batch=0 train mse <loss>=3.96066015625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=22, batch=0 train absolute_loss <loss>=1.6374681396484374\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:19.680] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 46, \"duration\": 66, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=22, train rmse <loss>=1.9716255970942322\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=22, train mse <loss>=3.8873074951171875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=22, train absolute_loss <loss>=1.6494512634277343\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.6113186, \"EndTime\": 1644664579.6807015, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 68.98713111877441, \"count\": 1, \"min\": 68.98713111877441, \"max\": 68.98713111877441}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.6116903, \"EndTime\": 1644664579.680893, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 22, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 186000.0, \"count\": 1, \"min\": 186000, \"max\": 186000}, \"Total Batches Seen\": {\"sum\": 93.0, \"count\": 1, \"min\": 93, \"max\": 93}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=115423.1462498452 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=23, batch=0 train rmse <loss>=1.9353216080441256\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=23, batch=0 train mse <loss>=3.7454697265625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=23, batch=0 train absolute_loss <loss>=1.5683887939453125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:19.733] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 48, \"duration\": 49, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=23, train rmse <loss>=1.9034761933505613\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=23, train mse <loss>=3.6232216186523436\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=23, train absolute_loss <loss>=1.5704989624023438\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.6807618, \"EndTime\": 1644664579.7355025, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 52.65998840332031, \"count\": 1, \"min\": 52.65998840332031, \"max\": 52.65998840332031}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.6828141, \"EndTime\": 1644664579.7361774, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 23, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 194000.0, \"count\": 1, \"min\": 194000, \"max\": 194000}, \"Total Batches Seen\": {\"sum\": 97.0, \"count\": 1, \"min\": 97, \"max\": 97}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 25.0, \"count\": 1, \"min\": 25, \"max\": 25}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=148948.76040395073 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=24, batch=0 train rmse <loss>=1.8925417066284496\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=24, batch=0 train mse <loss>=3.581714111328125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=24, batch=0 train absolute_loss <loss>=1.5059229736328126\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:19.786] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 48, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=24, train rmse <loss>=1.8475753979402894\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=24, train mse <loss>=3.413534851074219\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=24, train absolute_loss <loss>=1.4992483520507813\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.7358031, \"EndTime\": 1644664579.7878356, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 50.98915100097656, \"count\": 1, \"min\": 50.98915100097656, \"max\": 50.98915100097656}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.7368205, \"EndTime\": 1644664579.7884731, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 24, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 202000.0, \"count\": 1, \"min\": 202000, \"max\": 202000}, \"Total Batches Seen\": {\"sum\": 101.0, \"count\": 1, \"min\": 101, \"max\": 101}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=153824.15476654365 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=25, batch=0 train rmse <loss>=1.8599166096687978\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=25, batch=0 train mse <loss>=3.459289794921875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=25, batch=0 train absolute_loss <loss>=1.4497166748046875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:19.853] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 52, \"duration\": 63, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=25, train rmse <loss>=1.80231235749644\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=25, train mse <loss>=3.248329833984375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=25, train absolute_loss <loss>=1.435409912109375\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.7881408, \"EndTime\": 1644664579.855256, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 66.06459617614746, \"count\": 1, \"min\": 66.06459617614746, \"max\": 66.06459617614746}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.7891598, \"EndTime\": 1644664579.8569236, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 25, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 210000.0, \"count\": 1, \"min\": 210000, \"max\": 210000}, \"Total Batches Seen\": {\"sum\": 105.0, \"count\": 1, \"min\": 105, \"max\": 105}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 27.0, \"count\": 1, \"min\": 27, \"max\": 27}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=117322.36837504632 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=26, batch=0 train rmse <loss>=1.8356484479903485\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=26, batch=0 train mse <loss>=3.369605224609375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=26, batch=0 train absolute_loss <loss>=1.40433203125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:19.908] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 54, \"duration\": 48, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=26, train rmse <loss>=1.766107839814848\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=26, train mse <loss>=3.1191369018554687\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=26, train absolute_loss <loss>=1.394700927734375\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.8555918, \"EndTime\": 1644664579.909614, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 51.33557319641113, \"count\": 1, \"min\": 51.33557319641113, \"max\": 51.33557319641113}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.8582537, \"EndTime\": 1644664579.9099793, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 26, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 218000.0, \"count\": 1, \"min\": 218000, \"max\": 218000}, \"Total Batches Seen\": {\"sum\": 109.0, \"count\": 1, \"min\": 109, \"max\": 109}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=154303.75660475405 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=27, batch=0 train rmse <loss>=1.8180902476052996\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=27, batch=0 train mse <loss>=3.3054521484375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=27, batch=0 train absolute_loss <loss>=1.3978419189453124\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:19.985] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 69, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=27, train rmse <loss>=1.7374717567398759\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=27, train mse <loss>=3.01880810546875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #quality_metric: host=algo-1, epoch=27, train absolute_loss <loss>=1.3796881103515626\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.9096742, \"EndTime\": 1644664579.9861407, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 75.7145881652832, \"count\": 1, \"min\": 75.7145881652832, \"max\": 75.7145881652832}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.910399, \"EndTime\": 1644664579.9864335, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 27, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 226000.0, \"count\": 1, \"min\": 226000, \"max\": 226000}, \"Total Batches Seen\": {\"sum\": 113.0, \"count\": 1, \"min\": 113, \"max\": 113}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 29.0, \"count\": 1, \"min\": 29, \"max\": 29}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:19 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=105008.5497903236 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=28, batch=0 train rmse <loss>=1.8057877463217058\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=28, batch=0 train mse <loss>=3.260869384765625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=28, batch=0 train absolute_loss <loss>=1.395724853515625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:20.058] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 58, \"duration\": 69, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=28, train rmse <loss>=1.7150468095891818\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=28, train mse <loss>=2.9413855590820313\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=28, train absolute_loss <loss>=1.367437713623047\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.9862463, \"EndTime\": 1644664580.0585735, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 71.72632217407227, \"count\": 1, \"min\": 71.72632217407227, \"max\": 71.72632217407227}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664579.9868176, \"EndTime\": 1644664580.0587673, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 28, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 234000.0, \"count\": 1, \"min\": 234000, \"max\": 234000}, \"Total Batches Seen\": {\"sum\": 117.0, \"count\": 1, \"min\": 117, \"max\": 117}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=110871.696592013 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=29, batch=0 train rmse <loss>=1.7974977860947772\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=29, batch=0 train mse <loss>=3.230998291015625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=29, batch=0 train absolute_loss <loss>=1.3937835693359375\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:20.109] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 60, \"duration\": 48, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=29, train rmse <loss>=1.6976335637577606\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=29, train mse <loss>=2.881959716796875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=29, train absolute_loss <loss>=1.3566099548339843\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.0586376, \"EndTime\": 1644664580.1100492, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 50.96244812011719, \"count\": 1, \"min\": 50.96244812011719, \"max\": 50.96244812011719}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.059059, \"EndTime\": 1644664580.110354, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 29, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 242000.0, \"count\": 1, \"min\": 242000, \"max\": 242000}, \"Total Batches Seen\": {\"sum\": 121.0, \"count\": 1, \"min\": 121, \"max\": 121}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 31.0, \"count\": 1, \"min\": 31, \"max\": 31}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=155348.9078400326 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=30, batch=0 train rmse <loss>=1.7921884535742885\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=30, batch=0 train mse <loss>=3.211939453125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=30, batch=0 train absolute_loss <loss>=1.3920028076171875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:20.173] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 60, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=30, train rmse <loss>=1.6842012571280545\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=30, train mse <loss>=2.836533874511719\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=30, train absolute_loss <loss>=1.3470824279785156\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.110173, \"EndTime\": 1644664580.1742668, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 62.9878044128418, \"count\": 1, \"min\": 62.9878044128418, \"max\": 62.9878044128418}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.1112528, \"EndTime\": 1644664580.1745417, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 30, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 250000.0, \"count\": 1, \"min\": 250000, \"max\": 250000}, \"Total Batches Seen\": {\"sum\": 125.0, \"count\": 1, \"min\": 125, \"max\": 125}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=126106.5544197234 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=31, batch=0 train rmse <loss>=1.7890257911081102\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=31, batch=0 train mse <loss>=3.20061328125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=31, batch=0 train absolute_loss <loss>=1.3903671875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:20.230] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 64, \"duration\": 54, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=31, train rmse <loss>=1.6738851817142357\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=31, train mse <loss>=2.8018916015625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=31, train absolute_loss <loss>=1.3387353515625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.1743762, \"EndTime\": 1644664580.2314951, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 56.63752555847168, \"count\": 1, \"min\": 56.63752555847168, \"max\": 56.63752555847168}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.1748307, \"EndTime\": 1644664580.231785, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 31, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 258000.0, \"count\": 1, \"min\": 258000, \"max\": 258000}, \"Total Batches Seen\": {\"sum\": 129.0, \"count\": 1, \"min\": 129, \"max\": 129}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 33.0, \"count\": 1, \"min\": 33, \"max\": 33}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=140117.8086882446 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=32, batch=0 train rmse <loss>=1.7873529632488305\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=32, batch=0 train mse <loss>=3.194630615234375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=32, batch=0 train absolute_loss <loss>=1.3888623046875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:20.277] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 66, \"duration\": 44, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=32, train rmse <loss>=1.6659758842710346\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=32, train mse <loss>=2.775475646972656\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=32, train absolute_loss <loss>=1.331453125\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.2315888, \"EndTime\": 1644664580.2781181, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 46.021223068237305, \"count\": 1, \"min\": 46.021223068237305, \"max\": 46.021223068237305}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.2320719, \"EndTime\": 1644664580.278368, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 32, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 266000.0, \"count\": 1, \"min\": 266000, \"max\": 266000}, \"Total Batches Seen\": {\"sum\": 133.0, \"count\": 1, \"min\": 133, \"max\": 133}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=172267.4798876687 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=33, batch=0 train rmse <loss>=1.7866656296760246\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=33, batch=0 train mse <loss>=3.192174072265625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=33, batch=0 train absolute_loss <loss>=1.3874739990234375\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:20.341] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 62, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=33, train rmse <loss>=1.6599024806045648\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=33, train mse <loss>=2.7552762451171877\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=33, train absolute_loss <loss>=1.3251250610351561\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.278219, \"EndTime\": 1644664580.3428972, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 64.24117088317871, \"count\": 1, \"min\": 64.24117088317871, \"max\": 64.24117088317871}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.2786293, \"EndTime\": 1644664580.3433197, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 33, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 274000.0, \"count\": 1, \"min\": 274000, \"max\": 274000}, \"Total Batches Seen\": {\"sum\": 137.0, \"count\": 1, \"min\": 137, \"max\": 137}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 35.0, \"count\": 1, \"min\": 35, \"max\": 35}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=123159.03218228799 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=34, batch=0 train rmse <loss>=1.7865862366142238\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=34, batch=0 train mse <loss>=3.191890380859375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=34, batch=0 train absolute_loss <loss>=1.3861890869140625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:20.397] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 70, \"duration\": 52, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=34, train rmse <loss>=1.6552135244503767\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=34, train mse <loss>=2.7397318115234377\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=34, train absolute_loss <loss>=1.319646240234375\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.343094, \"EndTime\": 1644664580.3984041, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 54.5806884765625, \"count\": 1, \"min\": 54.5806884765625, \"max\": 54.5806884765625}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.3437946, \"EndTime\": 1644664580.398656, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 34, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 282000.0, \"count\": 1, \"min\": 282000, \"max\": 282000}, \"Total Batches Seen\": {\"sum\": 141.0, \"count\": 1, \"min\": 141, \"max\": 141}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=145466.98053904547 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=35, batch=0 train rmse <loss>=1.786840391300906\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=35, batch=0 train mse <loss>=3.192798583984375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=35, batch=0 train absolute_loss <loss>=1.3849949951171876\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:20.446] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 72, \"duration\": 45, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=35, train rmse <loss>=1.6515578989706488\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=35, train mse <loss>=2.727643493652344\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=35, train absolute_loss <loss>=1.3149180603027344\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.3984838, \"EndTime\": 1644664580.4481766, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 49.20840263366699, \"count\": 1, \"min\": 49.20840263366699, \"max\": 49.20840263366699}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.398938, \"EndTime\": 1644664580.449474, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 35, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 290000.0, \"count\": 1, \"min\": 290000, \"max\": 290000}, \"Total Batches Seen\": {\"sum\": 145.0, \"count\": 1, \"min\": 145, \"max\": 145}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 37.0, \"count\": 1, \"min\": 37, \"max\": 37}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=156780.29361467512 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=36, batch=0 train rmse <loss>=1.787234942651462\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=36, batch=0 train mse <loss>=3.194208740234375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=36, batch=0 train absolute_loss <loss>=1.3838798828125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:20.526] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 73, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=36, train rmse <loss>=1.6486662311689637\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=36, train mse <loss>=2.718100341796875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=36, train absolute_loss <loss>=1.310848602294922\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.4486663, \"EndTime\": 1644664580.528036, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 77.64172554016113, \"count\": 1, \"min\": 77.64172554016113, \"max\": 77.64172554016113}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.450363, \"EndTime\": 1644664580.5288389, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 36, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 298000.0, \"count\": 1, \"min\": 298000, \"max\": 298000}, \"Total Batches Seen\": {\"sum\": 149.0, \"count\": 1, \"min\": 149, \"max\": 149}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=101344.73801843596 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=37, batch=0 train rmse <loss>=1.787639649811484\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=37, batch=0 train mse <loss>=3.195655517578125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=37, batch=0 train absolute_loss <loss>=1.382833251953125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:20.588] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 76, \"duration\": 52, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=37, train rmse <loss>=1.64633481860797\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=37, train mse <loss>=2.7104183349609374\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=37, train absolute_loss <loss>=1.3073531799316407\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.5283816, \"EndTime\": 1644664580.589837, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 59.465646743774414, \"count\": 1, \"min\": 59.465646743774414, \"max\": 59.465646743774414}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.5303407, \"EndTime\": 1644664580.5906258, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 37, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 306000.0, \"count\": 1, \"min\": 306000, \"max\": 306000}, \"Total Batches Seen\": {\"sum\": 153.0, \"count\": 1, \"min\": 153, \"max\": 153}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 39.0, \"count\": 1, \"min\": 39, \"max\": 39}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=131625.2373256343 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=38, batch=0 train rmse <loss>=1.7879714194793495\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=38, batch=0 train mse <loss>=3.196841796875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=38, batch=0 train absolute_loss <loss>=1.381844970703125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:20.644] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 78, \"duration\": 51, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=38, train rmse <loss>=1.6444113673643792\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=38, train mse <loss>=2.7040887451171876\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=38, train absolute_loss <loss>=1.3043541870117188\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.5902116, \"EndTime\": 1644664580.6465006, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 54.95882034301758, \"count\": 1, \"min\": 54.95882034301758, \"max\": 54.95882034301758}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.5915112, \"EndTime\": 1644664580.647211, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 38, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 314000.0, \"count\": 1, \"min\": 314000, \"max\": 314000}, \"Total Batches Seen\": {\"sum\": 157.0, \"count\": 1, \"min\": 157, \"max\": 157}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=142325.1371103542 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=39, batch=0 train rmse <loss>=1.7881822343164582\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=39, batch=0 train mse <loss>=3.197595703125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=39, batch=0 train absolute_loss <loss>=1.3809061279296875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:20.701] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 50, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=39, train rmse <loss>=1.6427833087512542\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=39, train mse <loss>=2.698736999511719\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=39, train absolute_loss <loss>=1.3017809448242188\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.64684, \"EndTime\": 1644664580.701831, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 53.727149963378906, \"count\": 1, \"min\": 53.727149963378906, \"max\": 53.727149963378906}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.6480792, \"EndTime\": 1644664580.7020042, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 39, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 322000.0, \"count\": 1, \"min\": 322000, \"max\": 322000}, \"Total Batches Seen\": {\"sum\": 161.0, \"count\": 1, \"min\": 161, \"max\": 161}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 41.0, \"count\": 1, \"min\": 41, \"max\": 41}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=148039.91917338016 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=40, batch=0 train rmse <loss>=1.7882483819063348\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=40, batch=0 train mse <loss>=3.197832275390625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=40, batch=0 train absolute_loss <loss>=1.380008544921875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:20.755] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 82, \"duration\": 52, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=40, train rmse <loss>=1.641368079488352\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=40, train mse <loss>=2.6940891723632814\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=40, train absolute_loss <loss>=1.2995698547363281\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.7018912, \"EndTime\": 1644664580.7566512, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 54.44765090942383, \"count\": 1, \"min\": 54.44765090942383, \"max\": 54.44765090942383}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.7021806, \"EndTime\": 1644664580.7568169, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 40, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 330000.0, \"count\": 1, \"min\": 330000, \"max\": 330000}, \"Total Batches Seen\": {\"sum\": 165.0, \"count\": 1, \"min\": 165, \"max\": 165}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=146183.74459779728 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=41, batch=0 train rmse <loss>=1.788163324803379\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=41, batch=0 train mse <loss>=3.197528076171875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=41, batch=0 train absolute_loss <loss>=1.37914501953125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:20.799] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 84, \"duration\": 41, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=41, train rmse <loss>=1.6401058516418519\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=41, train mse <loss>=2.689947204589844\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=41, train absolute_loss <loss>=1.2976642150878905\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.756707, \"EndTime\": 1644664580.8011248, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 44.10886764526367, \"count\": 1, \"min\": 44.10886764526367, \"max\": 44.10886764526367}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.756989, \"EndTime\": 1644664580.8016665, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 41, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 338000.0, \"count\": 1, \"min\": 338000, \"max\": 338000}, \"Total Batches Seen\": {\"sum\": 169.0, \"count\": 1, \"min\": 169, \"max\": 169}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 43.0, \"count\": 1, \"min\": 43, \"max\": 43}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=178593.11695638753 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=42, batch=0 train rmse <loss>=1.7879315475571205\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=42, batch=0 train mse <loss>=3.19669921875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=42, batch=0 train absolute_loss <loss>=1.3783092041015625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:20.840] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 36, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=42, train rmse <loss>=1.638953166038844\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=42, train mse <loss>=2.68616748046875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=42, train absolute_loss <loss>=1.2960135192871094\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.801462, \"EndTime\": 1644664580.841728, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 39.51215744018555, \"count\": 1, \"min\": 39.51215744018555, \"max\": 39.51215744018555}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.8021894, \"EndTime\": 1644664580.8419416, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 42, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 346000.0, \"count\": 1, \"min\": 346000, \"max\": 346000}, \"Total Batches Seen\": {\"sum\": 173.0, \"count\": 1, \"min\": 173, \"max\": 173}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=200700.00657945892 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=43, batch=0 train rmse <loss>=1.7875641924662453\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=43, batch=0 train mse <loss>=3.1953857421875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=43, batch=0 train absolute_loss <loss>=1.3774952392578126\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:20.893] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 88, \"duration\": 49, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=43, train rmse <loss>=1.6378789332250243\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=43, train mse <loss>=2.682647399902344\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=43, train absolute_loss <loss>=1.2945735778808594\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.8417852, \"EndTime\": 1644664580.89383, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 51.644325256347656, \"count\": 1, \"min\": 51.644325256347656, \"max\": 51.644325256347656}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.8421621, \"EndTime\": 1644664580.8940067, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 43, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 354000.0, \"count\": 1, \"min\": 354000, \"max\": 354000}, \"Total Batches Seen\": {\"sum\": 177.0, \"count\": 1, \"min\": 177, \"max\": 177}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 45.0, \"count\": 1, \"min\": 45, \"max\": 45}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=154038.1211208638 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=44, batch=0 train rmse <loss>=1.7870760669710244\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=44, batch=0 train mse <loss>=3.193640869140625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=44, batch=0 train absolute_loss <loss>=1.376698486328125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:20.939] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 90, \"duration\": 44, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=44, train rmse <loss>=1.6368606553936658\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=44, train mse <loss>=2.679312805175781\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=44, train absolute_loss <loss>=1.2933058166503906\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.8938856, \"EndTime\": 1644664580.9403546, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 46.15378379821777, \"count\": 1, \"min\": 46.15378379821777, \"max\": 46.15378379821777}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.8941786, \"EndTime\": 1644664580.9405773, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 44, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 362000.0, \"count\": 1, \"min\": 362000, \"max\": 362000}, \"Total Batches Seen\": {\"sum\": 181.0, \"count\": 1, \"min\": 181, \"max\": 181}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=171977.87903172072 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=45, batch=0 train rmse <loss>=1.7864834030067982\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=45, batch=0 train mse <loss>=3.19152294921875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=45, batch=0 train absolute_loss <loss>=1.37591455078125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:20.983] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 41, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=45, train rmse <loss>=1.6358821138845379\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=45, train mse <loss>=2.676110290527344\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=45, train absolute_loss <loss>=1.292176788330078\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.9404204, \"EndTime\": 1644664580.9834852, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 42.65189170837402, \"count\": 1, \"min\": 42.65189170837402, \"max\": 42.65189170837402}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.9408145, \"EndTime\": 1644664580.9836345, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 45, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 370000.0, \"count\": 1, \"min\": 370000, \"max\": 370000}, \"Total Batches Seen\": {\"sum\": 185.0, \"count\": 1, \"min\": 185, \"max\": 185}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 47.0, \"count\": 1, \"min\": 47, \"max\": 47}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=186472.5607553503 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=46, batch=0 train rmse <loss>=1.7858023652942745\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=46, batch=0 train mse <loss>=3.189090087890625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:20 INFO 140146954667840] #quality_metric: host=algo-1, epoch=46, batch=0 train absolute_loss <loss>=1.3751400146484376\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.021] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 94, \"duration\": 36, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=46, train rmse <loss>=1.6349316018639548\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=46, train mse <loss>=2.6730013427734374\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=46, train absolute_loss <loss>=1.2911582641601562\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.9835305, \"EndTime\": 1644664581.022048, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 38.24281692504883, \"count\": 1, \"min\": 38.24281692504883, \"max\": 38.24281692504883}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664580.9837801, \"EndTime\": 1644664581.0223403, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 46, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 378000.0, \"count\": 1, \"min\": 378000, \"max\": 378000}, \"Total Batches Seen\": {\"sum\": 189.0, \"count\": 1, \"min\": 189, \"max\": 189}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=206850.32302608868 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=47, batch=0 train rmse <loss>=1.7850480341926376\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=47, batch=0 train mse <loss>=3.186396484375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=47, batch=0 train absolute_loss <loss>=1.37437158203125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.065] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 96, \"duration\": 42, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=47, train rmse <loss>=1.63400042761957\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=47, train mse <loss>=2.6699573974609376\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=47, train absolute_loss <loss>=1.290225860595703\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.0221357, \"EndTime\": 1644664581.0662098, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 43.637990951538086, \"count\": 1, \"min\": 43.637990951538086, \"max\": 43.637990951538086}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.022554, \"EndTime\": 1644664581.0663304, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 47, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 386000.0, \"count\": 1, \"min\": 386000, \"max\": 386000}, \"Total Batches Seen\": {\"sum\": 193.0, \"count\": 1, \"min\": 193, \"max\": 193}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 49.0, \"count\": 1, \"min\": 49, \"max\": 49}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=182446.31730047739 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=48, batch=0 train rmse <loss>=1.7842342052597102\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=48, batch=0 train mse <loss>=3.18349169921875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=48, batch=0 train absolute_loss <loss>=1.37360693359375\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.118] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 51, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=48, train rmse <loss>=1.6330822145658344\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=48, train mse <loss>=2.66695751953125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=48, train absolute_loss <loss>=1.289359375\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.0662506, \"EndTime\": 1644664581.1195095, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 53.00474166870117, \"count\": 1, \"min\": 53.00474166870117, \"max\": 53.00474166870117}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 49.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.0664756, \"EndTime\": 1644664581.1198914, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 48, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 394000.0, \"count\": 1, \"min\": 394000, \"max\": 394000}, \"Total Batches Seen\": {\"sum\": 197.0, \"count\": 1, \"min\": 197, \"max\": 197}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=149448.96913874426 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=49, batch=0 train rmse <loss>=1.7833730494749969\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=49, batch=0 train mse <loss>=3.18041943359375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=49, batch=0 train absolute_loss <loss>=1.372843994140625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.165] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 100, \"duration\": 44, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=49, train rmse <loss>=1.6321722728085415\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=49, train mse <loss>=2.663986328125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=49, train absolute_loss <loss>=1.2885419616699219\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.119718, \"EndTime\": 1644664581.1665397, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 46.33069038391113, \"count\": 1, \"min\": 46.33069038391113, \"max\": 46.33069038391113}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.1201582, \"EndTime\": 1644664581.1668074, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 49, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 402000.0, \"count\": 1, \"min\": 402000, \"max\": 402000}, \"Total Batches Seen\": {\"sum\": 201.0, \"count\": 1, \"min\": 201, \"max\": 201}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 51.0, \"count\": 1, \"min\": 51, \"max\": 51}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=170917.03341483293 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=50, batch=0 train rmse <loss>=1.7824747023318803\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=50, batch=0 train mse <loss>=3.177216064453125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=50, batch=0 train absolute_loss <loss>=1.3720809326171874\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.210] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 102, \"duration\": 41, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=50, train rmse <loss>=1.631266968152889\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=50, train mse <loss>=2.661031921386719\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=50, train absolute_loss <loss>=1.287759979248047\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.1666005, \"EndTime\": 1644664581.2110996, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 43.86425018310547, \"count\": 1, \"min\": 43.86425018310547, \"max\": 43.86425018310547}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 51.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.1672068, \"EndTime\": 1644664581.2113912, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 50, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 410000.0, \"count\": 1, \"min\": 410000, \"max\": 410000}, \"Total Batches Seen\": {\"sum\": 205.0, \"count\": 1, \"min\": 205, \"max\": 205}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 52.0, \"count\": 1, \"min\": 52, \"max\": 52}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=180204.46611744235 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=51, batch=0 train rmse <loss>=1.7815478095342403\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=51, batch=0 train mse <loss>=3.17391259765625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=51, batch=0 train absolute_loss <loss>=1.371316650390625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.255] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 41, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=51, train rmse <loss>=1.630363594177308\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=51, train mse <loss>=2.65808544921875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=51, train absolute_loss <loss>=1.2870020751953124\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.2111862, \"EndTime\": 1644664581.255564, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 43.8082218170166, \"count\": 1, \"min\": 43.8082218170166, \"max\": 43.8082218170166}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.2117267, \"EndTime\": 1644664581.2558017, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 51, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 418000.0, \"count\": 1, \"min\": 418000, \"max\": 418000}, \"Total Batches Seen\": {\"sum\": 209.0, \"count\": 1, \"min\": 209, \"max\": 209}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 53.0, \"count\": 1, \"min\": 53, \"max\": 53}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=180918.6108579963 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=52, batch=0 train rmse <loss>=1.7805994563146774\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=52, batch=0 train mse <loss>=3.170534423828125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=52, batch=0 train absolute_loss <loss>=1.3705499267578125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.297] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 106, \"duration\": 40, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=52, train rmse <loss>=1.6294601314006485\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=52, train mse <loss>=2.655140319824219\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=52, train absolute_loss <loss>=1.286259521484375\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.255637, \"EndTime\": 1644664581.29821, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 42.114973068237305, \"count\": 1, \"min\": 42.114973068237305, \"max\": 42.114973068237305}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 53.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.2560651, \"EndTime\": 1644664581.2984693, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 52, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 426000.0, \"count\": 1, \"min\": 426000, \"max\": 426000}, \"Total Batches Seen\": {\"sum\": 213.0, \"count\": 1, \"min\": 213, \"max\": 213}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 54.0, \"count\": 1, \"min\": 54, \"max\": 54}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=188035.84257510633 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=53, batch=0 train rmse <loss>=1.7796353015830588\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=53, batch=0 train mse <loss>=3.167101806640625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=53, batch=0 train absolute_loss <loss>=1.369780029296875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.337] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 108, \"duration\": 37, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=53, train rmse <loss>=1.6285549868563982\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=53, train mse <loss>=2.6521913452148436\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=53, train absolute_loss <loss>=1.2855252990722656\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.2982814, \"EndTime\": 1644664581.3383572, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 39.5665168762207, \"count\": 1, \"min\": 39.5665168762207, \"max\": 39.5665168762207}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.2987626, \"EndTime\": 1644664581.338593, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 53, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 434000.0, \"count\": 1, \"min\": 434000, \"max\": 434000}, \"Total Batches Seen\": {\"sum\": 217.0, \"count\": 1, \"min\": 217, \"max\": 217}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 55.0, \"count\": 1, \"min\": 55, \"max\": 55}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=200216.19299365717 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=54, batch=0 train rmse <loss>=1.7786597806310163\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=54, batch=0 train mse <loss>=3.163630615234375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=54, batch=0 train absolute_loss <loss>=1.3690064697265625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.402] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 59, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=54, train rmse <loss>=1.6276471077668755\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=54, train mse <loss>=2.649235107421875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=54, train absolute_loss <loss>=1.2847941284179687\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.3384311, \"EndTime\": 1644664581.4030347, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 64.12267684936523, \"count\": 1, \"min\": 64.12267684936523, \"max\": 64.12267684936523}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.3388813, \"EndTime\": 1644664581.4033556, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 54, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 442000.0, \"count\": 1, \"min\": 442000, \"max\": 442000}, \"Total Batches Seen\": {\"sum\": 221.0, \"count\": 1, \"min\": 221, \"max\": 221}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 56.0, \"count\": 1, \"min\": 56, \"max\": 56}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=123712.55286123534 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=55, batch=0 train rmse <loss>=1.7776762395048205\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=55, batch=0 train mse <loss>=3.1601328125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=55, batch=0 train absolute_loss <loss>=1.3682286376953126\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.443] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 112, \"duration\": 37, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=55, train rmse <loss>=1.6267356641133917\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=55, train mse <loss>=2.6462689208984376\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=55, train absolute_loss <loss>=1.284062042236328\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.40314, \"EndTime\": 1644664581.4440463, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 40.313005447387695, \"count\": 1, \"min\": 40.313005447387695, \"max\": 40.313005447387695}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.4037106, \"EndTime\": 1644664581.4445443, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 55, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 450000.0, \"count\": 1, \"min\": 450000, \"max\": 450000}, \"Total Batches Seen\": {\"sum\": 225.0, \"count\": 1, \"min\": 225, \"max\": 225}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 57.0, \"count\": 1, \"min\": 57, \"max\": 57}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=194534.23466252335 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=56, batch=0 train rmse <loss>=1.776687344448053\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=56, batch=0 train mse <loss>=3.156617919921875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=56, batch=0 train absolute_loss <loss>=1.367446533203125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.486] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 114, \"duration\": 40, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=56, train rmse <loss>=1.6258201618661483\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=56, train mse <loss>=2.6432911987304686\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=56, train absolute_loss <loss>=1.283326171875\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.444268, \"EndTime\": 1644664581.4876788, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 42.71578788757324, \"count\": 1, \"min\": 42.71578788757324, \"max\": 42.71578788757324}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 57.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.444934, \"EndTime\": 1644664581.4881985, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 56, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 458000.0, \"count\": 1, \"min\": 458000, \"max\": 458000}, \"Total Batches Seen\": {\"sum\": 229.0, \"count\": 1, \"min\": 229, \"max\": 229}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 58.0, \"count\": 1, \"min\": 58, \"max\": 58}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=184450.14182369885 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=57, batch=0 train rmse <loss>=1.7756949426342563\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=57, batch=0 train mse <loss>=3.153092529296875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=57, batch=0 train absolute_loss <loss>=1.3666597900390625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.537] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 116, \"duration\": 48, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=57, train rmse <loss>=1.6249002561034804\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=57, train mse <loss>=2.640300842285156\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=57, train absolute_loss <loss>=1.2825846252441406\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.487933, \"EndTime\": 1644664581.5381813, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 49.73244667053223, \"count\": 1, \"min\": 49.73244667053223, \"max\": 49.73244667053223}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 58.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.4884253, \"EndTime\": 1644664581.5383914, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 57, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 466000.0, \"count\": 1, \"min\": 466000, \"max\": 466000}, \"Total Batches Seen\": {\"sum\": 233.0, \"count\": 1, \"min\": 233, \"max\": 233}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 59.0, \"count\": 1, \"min\": 59, \"max\": 59}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=159704.67818165376 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=58, batch=0 train rmse <loss>=1.7747002662861833\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=58, batch=0 train mse <loss>=3.14956103515625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=58, batch=0 train absolute_loss <loss>=1.36586865234375\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.577] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 118, \"duration\": 37, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=58, train rmse <loss>=1.6239756762557476\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=58, train mse <loss>=2.6372969970703126\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=58, train absolute_loss <loss>=1.2818360290527344\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.5382397, \"EndTime\": 1644664581.5781264, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 39.48831558227539, \"count\": 1, \"min\": 39.48831558227539, \"max\": 39.48831558227539}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 59.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.5386138, \"EndTime\": 1644664581.5783584, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 58, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 474000.0, \"count\": 1, \"min\": 474000, \"max\": 474000}, \"Total Batches Seen\": {\"sum\": 237.0, \"count\": 1, \"min\": 237, \"max\": 237}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=200698.80613441157 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=59, batch=0 train rmse <loss>=1.7737044127334183\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=59, batch=0 train mse <loss>=3.14602734375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=59, batch=0 train absolute_loss <loss>=1.36507275390625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.616] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 120, \"duration\": 36, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=59, train rmse <loss>=1.6230464707430303\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=59, train mse <loss>=2.634279846191406\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=59, train absolute_loss <loss>=1.2810796508789062\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.5781934, \"EndTime\": 1644664581.6171656, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 38.51437568664551, \"count\": 1, \"min\": 38.51437568664551, \"max\": 38.51437568664551}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.5786102, \"EndTime\": 1644664581.617408, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 59, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 482000.0, \"count\": 1, \"min\": 482000, \"max\": 482000}, \"Total Batches Seen\": {\"sum\": 241.0, \"count\": 1, \"min\": 241, \"max\": 241}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 61.0, \"count\": 1, \"min\": 61, \"max\": 61}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=205504.91799262608 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=60, batch=0 train rmse <loss>=1.7727077931576527\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=60, batch=0 train mse <loss>=3.142492919921875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=60, batch=0 train absolute_loss <loss>=1.3642724609375\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.665] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 122, \"duration\": 45, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=60, train rmse <loss>=1.6221126316160779\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=60, train mse <loss>=2.6312493896484375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=60, train absolute_loss <loss>=1.2803151550292968\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.61724, \"EndTime\": 1644664581.6662073, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 48.505306243896484, \"count\": 1, \"min\": 48.505306243896484, \"max\": 48.505306243896484}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 61.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.6176715, \"EndTime\": 1644664581.6667192, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 60, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 490000.0, \"count\": 1, \"min\": 490000, \"max\": 490000}, \"Total Batches Seen\": {\"sum\": 245.0, \"count\": 1, \"min\": 245, \"max\": 245}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 62.0, \"count\": 1, \"min\": 62, \"max\": 62}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=162709.4552983906 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=61, batch=0 train rmse <loss>=1.7717107507646388\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=61, batch=0 train mse <loss>=3.138958984375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=61, batch=0 train absolute_loss <loss>=1.3634677734375\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.709] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 124, \"duration\": 40, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=61, train rmse <loss>=1.6211743391113027\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=61, train mse <loss>=2.628206237792969\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=61, train absolute_loss <loss>=1.2795426635742186\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.6665168, \"EndTime\": 1644664581.7102234, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 43.19906234741211, \"count\": 1, \"min\": 43.19906234741211, \"max\": 43.19906234741211}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 62.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.667, \"EndTime\": 1644664581.7104568, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 61, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 498000.0, \"count\": 1, \"min\": 498000, \"max\": 498000}, \"Total Batches Seen\": {\"sum\": 249.0, \"count\": 1, \"min\": 249, \"max\": 249}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 63.0, \"count\": 1, \"min\": 63, \"max\": 63}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=183576.23836045124 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=62, batch=0 train rmse <loss>=1.7707134227171333\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=62, batch=0 train mse <loss>=3.135426025390625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=62, batch=0 train absolute_loss <loss>=1.3626588134765625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.753] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 126, \"duration\": 41, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=62, train rmse <loss>=1.620231717338941\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=62, train mse <loss>=2.6251508178710936\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=62, train absolute_loss <loss>=1.2787622985839844\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.7102942, \"EndTime\": 1644664581.7542317, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 43.505191802978516, \"count\": 1, \"min\": 43.505191802978516, \"max\": 43.505191802978516}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 63.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.7107038, \"EndTime\": 1644664581.7544358, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 62, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 506000.0, \"count\": 1, \"min\": 506000, \"max\": 506000}, \"Total Batches Seen\": {\"sum\": 253.0, \"count\": 1, \"min\": 253, \"max\": 253}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 64.0, \"count\": 1, \"min\": 64, \"max\": 64}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=182445.3252861376 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=63, batch=0 train rmse <loss>=1.769715808532192\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=63, batch=0 train mse <loss>=3.13189404296875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=63, batch=0 train absolute_loss <loss>=1.361845703125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.808] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 128, \"duration\": 52, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=63, train rmse <loss>=1.619284947201812\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=63, train mse <loss>=2.622083740234375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=63, train absolute_loss <loss>=1.2779744262695312\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.754293, \"EndTime\": 1644664581.810037, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 55.3441047668457, \"count\": 1, \"min\": 55.3441047668457, \"max\": 55.3441047668457}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 64.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.754666, \"EndTime\": 1644664581.8106632, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 63, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 514000.0, \"count\": 1, \"min\": 514000, \"max\": 514000}, \"Total Batches Seen\": {\"sum\": 257.0, \"count\": 1, \"min\": 257, \"max\": 257}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 65.0, \"count\": 1, \"min\": 65, \"max\": 65}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=141936.2196231044 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=64, batch=0 train rmse <loss>=1.7687178387093714\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=64, batch=0 train mse <loss>=3.12836279296875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=64, batch=0 train absolute_loss <loss>=1.361028564453125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.859] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 130, \"duration\": 46, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=64, train rmse <loss>=1.6183343231376244\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=64, train mse <loss>=2.6190059814453126\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=64, train absolute_loss <loss>=1.2771795043945313\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.810327, \"EndTime\": 1644664581.8602858, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 48.93827438354492, \"count\": 1, \"min\": 48.93827438354492, \"max\": 48.93827438354492}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.8113236, \"EndTime\": 1644664581.8604343, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 64, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 522000.0, \"count\": 1, \"min\": 522000, \"max\": 522000}, \"Total Batches Seen\": {\"sum\": 261.0, \"count\": 1, \"min\": 261, \"max\": 261}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 66.0, \"count\": 1, \"min\": 66, \"max\": 66}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=162587.2525172257 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=65, batch=0 train rmse <loss>=1.7677192364253225\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=65, batch=0 train mse <loss>=3.124831298828125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=65, batch=0 train absolute_loss <loss>=1.360207763671875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.913] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 132, \"duration\": 51, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=65, train rmse <loss>=1.6173799892990999\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=65, train mse <loss>=2.6159180297851563\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=65, train absolute_loss <loss>=1.276378173828125\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.8603384, \"EndTime\": 1644664581.9138849, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 53.27200889587402, \"count\": 1, \"min\": 53.27200889587402, \"max\": 53.27200889587402}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 66.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.860589, \"EndTime\": 1644664581.9140637, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 65, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 530000.0, \"count\": 1, \"min\": 530000, \"max\": 530000}, \"Total Batches Seen\": {\"sum\": 265.0, \"count\": 1, \"min\": 265, \"max\": 265}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 67.0, \"count\": 1, \"min\": 67, \"max\": 67}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=149362.48670159493 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=66, batch=0 train rmse <loss>=1.7667200006075878\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=66, batch=0 train mse <loss>=3.121299560546875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=66, batch=0 train absolute_loss <loss>=1.3593831787109374\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.958] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 134, \"duration\": 43, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=66, train rmse <loss>=1.6164221845517146\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=66, train mse <loss>=2.6128206787109374\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=66, train absolute_loss <loss>=1.2755707702636718\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.9139535, \"EndTime\": 1644664581.9598722, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 45.61209678649902, \"count\": 1, \"min\": 45.61209678649902, \"max\": 45.61209678649902}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 67.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.9142337, \"EndTime\": 1644664581.9602304, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 66, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 538000.0, \"count\": 1, \"min\": 538000, \"max\": 538000}, \"Total Batches Seen\": {\"sum\": 269.0, \"count\": 1, \"min\": 269, \"max\": 269}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 68.0, \"count\": 1, \"min\": 68, \"max\": 68}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=173234.23544353465 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=67, batch=0 train rmse <loss>=1.7657198536468066\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=67, batch=0 train mse <loss>=3.1177666015625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=67, batch=0 train absolute_loss <loss>=1.35855517578125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:21.998] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 136, \"duration\": 36, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=67, train rmse <loss>=1.6154610916311847\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=67, train mse <loss>=2.609714538574219\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #quality_metric: host=algo-1, epoch=67, train absolute_loss <loss>=1.2747577819824218\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.9599938, \"EndTime\": 1644664581.9990275, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 38.405656814575195, \"count\": 1, \"min\": 38.405656814575195, \"max\": 38.405656814575195}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #progress_metric: host=algo-1, completed 68.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.960595, \"EndTime\": 1644664581.9992597, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 67, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 546000.0, \"count\": 1, \"min\": 546000, \"max\": 546000}, \"Total Batches Seen\": {\"sum\": 273.0, \"count\": 1, \"min\": 273, \"max\": 273}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 69.0, \"count\": 1, \"min\": 69, \"max\": 69}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:21 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=206224.8445067237 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=68, batch=0 train rmse <loss>=1.764718724821147\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=68, batch=0 train mse <loss>=3.114232177734375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=68, batch=0 train absolute_loss <loss>=1.3577236328125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.041] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 138, \"duration\": 40, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=68, train rmse <loss>=1.614496988198598\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=68, train mse <loss>=2.6066005249023436\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=68, train absolute_loss <loss>=1.2739397888183595\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.9990966, \"EndTime\": 1644664582.0422504, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 42.68765449523926, \"count\": 1, \"min\": 42.68765449523926, \"max\": 42.68765449523926}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 69.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664581.9995368, \"EndTime\": 1644664582.0425792, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 68, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 554000.0, \"count\": 1, \"min\": 554000, \"max\": 554000}, \"Total Batches Seen\": {\"sum\": 277.0, \"count\": 1, \"min\": 277, \"max\": 277}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 70.0, \"count\": 1, \"min\": 70, \"max\": 70}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=185288.39877852815 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=69, batch=0 train rmse <loss>=1.76371633561069\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=69, batch=0 train mse <loss>=3.1106953125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=69, batch=0 train absolute_loss <loss>=1.3568890380859375\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.083] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 140, \"duration\": 39, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=69, train rmse <loss>=1.6135299634252516\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=69, train mse <loss>=2.6034789428710936\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=69, train absolute_loss <loss>=1.2731171875\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.042326, \"EndTime\": 1644664582.084682, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 41.778564453125, \"count\": 1, \"min\": 41.778564453125, \"max\": 41.778564453125}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.0428421, \"EndTime\": 1644664582.0856733, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 69, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 562000.0, \"count\": 1, \"min\": 562000, \"max\": 562000}, \"Total Batches Seen\": {\"sum\": 281.0, \"count\": 1, \"min\": 281, \"max\": 281}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 71.0, \"count\": 1, \"min\": 71, \"max\": 71}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=186159.09368307758 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=70, batch=0 train rmse <loss>=1.7627126146138372\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=70, batch=0 train mse <loss>=3.10715576171875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=70, batch=0 train absolute_loss <loss>=1.35605126953125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.130] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 142, \"duration\": 43, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=70, train rmse <loss>=1.612560220229678\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=70, train mse <loss>=2.6003504638671875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=70, train absolute_loss <loss>=1.2722904052734374\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.085464, \"EndTime\": 1644664582.1307263, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 44.74043846130371, \"count\": 1, \"min\": 44.74043846130371, \"max\": 44.74043846130371}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 71.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.0859606, \"EndTime\": 1644664582.1309211, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 70, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 570000.0, \"count\": 1, \"min\": 570000, \"max\": 570000}, \"Total Batches Seen\": {\"sum\": 285.0, \"count\": 1, \"min\": 285, \"max\": 285}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=177242.21135256768 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=71, batch=0 train rmse <loss>=1.76170735168153\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=71, batch=0 train mse <loss>=3.10361279296875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=71, batch=0 train absolute_loss <loss>=1.35521044921875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.174] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 144, \"duration\": 41, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=71, train rmse <loss>=1.611587905195331\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=71, train mse <loss>=2.597215576171875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=71, train absolute_loss <loss>=1.2714596557617188\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.1307929, \"EndTime\": 1644664582.1746593, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 43.466806411743164, \"count\": 1, \"min\": 43.466806411743164, \"max\": 43.466806411743164}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 72.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.1311755, \"EndTime\": 1644664582.174795, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 71, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 578000.0, \"count\": 1, \"min\": 578000, \"max\": 578000}, \"Total Batches Seen\": {\"sum\": 289.0, \"count\": 1, \"min\": 289, \"max\": 289}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 73.0, \"count\": 1, \"min\": 73, \"max\": 73}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=183061.45251396648 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=72, batch=0 train rmse <loss>=1.760700752164284\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=72, batch=0 train mse <loss>=3.100067138671875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=72, batch=0 train absolute_loss <loss>=1.35436669921875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.216] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 146, \"duration\": 39, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=72, train rmse <loss>=1.6106131273512252\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=72, train mse <loss>=2.594074645996094\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=72, train absolute_loss <loss>=1.270625244140625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.1747036, \"EndTime\": 1644664582.2165797, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 41.61715507507324, \"count\": 1, \"min\": 41.61715507507324, \"max\": 41.61715507507324}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 73.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.1749382, \"EndTime\": 1644664582.216837, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 72, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 586000.0, \"count\": 1, \"min\": 586000, \"max\": 586000}, \"Total Batches Seen\": {\"sum\": 293.0, \"count\": 1, \"min\": 293, \"max\": 293}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 74.0, \"count\": 1, \"min\": 74, \"max\": 74}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=190455.3978885231 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=73, batch=0 train rmse <loss>=1.7596924669171599\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=73, batch=0 train mse <loss>=3.096517578125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=73, batch=0 train absolute_loss <loss>=1.3535201416015625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.261] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 148, \"duration\": 43, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=73, train rmse <loss>=1.6096360718160507\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=73, train mse <loss>=2.5909282836914063\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=73, train absolute_loss <loss>=1.2697875671386718\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.21666, \"EndTime\": 1644664582.2619348, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 44.86823081970215, \"count\": 1, \"min\": 44.86823081970215, \"max\": 44.86823081970215}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 74.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.2170398, \"EndTime\": 1644664582.2622635, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 73, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 594000.0, \"count\": 1, \"min\": 594000, \"max\": 594000}, \"Total Batches Seen\": {\"sum\": 297.0, \"count\": 1, \"min\": 297, \"max\": 297}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 75.0, \"count\": 1, \"min\": 75, \"max\": 75}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=176306.26474498078 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=74, batch=0 train rmse <loss>=1.7586825624508675\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=74, batch=0 train mse <loss>=3.09296435546875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=74, batch=0 train absolute_loss <loss>=1.3526708984375\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.304] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 150, \"duration\": 38, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=74, train rmse <loss>=1.6086568292938037\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=74, train mse <loss>=2.587776794433594\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=74, train absolute_loss <loss>=1.2689467163085937\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.2620332, \"EndTime\": 1644664582.3058014, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 43.15519332885742, \"count\": 1, \"min\": 43.15519332885742, \"max\": 43.15519332885742}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.2626183, \"EndTime\": 1644664582.3060524, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 74, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 602000.0, \"count\": 1, \"min\": 602000, \"max\": 602000}, \"Total Batches Seen\": {\"sum\": 301.0, \"count\": 1, \"min\": 301, \"max\": 301}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 76.0, \"count\": 1, \"min\": 76, \"max\": 76}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=183596.32746413368 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=75, batch=0 train rmse <loss>=1.7576710359743444\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=75, batch=0 train mse <loss>=3.089407470703125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=75, batch=0 train absolute_loss <loss>=1.3518189697265626\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.346] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 152, \"duration\": 39, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=75, train rmse <loss>=1.6076754337530224\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=75, train mse <loss>=2.584620300292969\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=75, train absolute_loss <loss>=1.2681029052734376\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.3058786, \"EndTime\": 1644664582.3474636, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 41.09907150268555, \"count\": 1, \"min\": 41.09907150268555, \"max\": 41.09907150268555}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 76.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.3063216, \"EndTime\": 1644664582.3476987, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 75, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 610000.0, \"count\": 1, \"min\": 610000, \"max\": 610000}, \"Total Batches Seen\": {\"sum\": 305.0, \"count\": 1, \"min\": 305, \"max\": 305}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 77.0, \"count\": 1, \"min\": 77, \"max\": 77}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=192696.48712750745 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=76, batch=0 train rmse <loss>=1.756657884685611\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=76, batch=0 train mse <loss>=3.085846923828125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=76, batch=0 train absolute_loss <loss>=1.35096435546875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.389] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 154, \"duration\": 39, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=76, train rmse <loss>=1.6066920142067131\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=76, train mse <loss>=2.581459228515625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=76, train absolute_loss <loss>=1.2672562561035157\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.347531, \"EndTime\": 1644664582.3900666, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 42.08707809448242, \"count\": 1, \"min\": 42.08707809448242, \"max\": 42.08707809448242}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 77.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.3479526, \"EndTime\": 1644664582.3903153, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 76, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 618000.0, \"count\": 1, \"min\": 618000, \"max\": 618000}, \"Total Batches Seen\": {\"sum\": 309.0, \"count\": 1, \"min\": 309, \"max\": 309}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 78.0, \"count\": 1, \"min\": 78, \"max\": 78}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=188267.95042277544 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=77, batch=0 train rmse <loss>=1.755643036241458\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=77, batch=0 train mse <loss>=3.082282470703125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=77, batch=0 train absolute_loss <loss>=1.3501072998046875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.442] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 156, \"duration\": 48, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=77, train rmse <loss>=1.6057066239531526\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=77, train mse <loss>=2.578293762207031\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=77, train absolute_loss <loss>=1.2664068908691406\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.3901422, \"EndTime\": 1644664582.4430232, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 52.388668060302734, \"count\": 1, \"min\": 52.388668060302734, \"max\": 52.388668060302734}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 78.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.3905866, \"EndTime\": 1644664582.4434514, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 77, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 626000.0, \"count\": 1, \"min\": 626000, \"max\": 626000}, \"Total Batches Seen\": {\"sum\": 313.0, \"count\": 1, \"min\": 313, \"max\": 313}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 79.0, \"count\": 1, \"min\": 79, \"max\": 79}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=150710.92925382117 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=78, batch=0 train rmse <loss>=1.7546266268381359\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=78, batch=0 train mse <loss>=3.078714599609375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=78, batch=0 train absolute_loss <loss>=1.349247802734375\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.507] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 158, \"duration\": 61, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=78, train rmse <loss>=1.6047193924837082\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=78, train mse <loss>=2.575124328613281\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=78, train absolute_loss <loss>=1.2655549621582032\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.4431584, \"EndTime\": 1644664582.5080323, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 63.94600868225098, \"count\": 1, \"min\": 63.94600868225098, \"max\": 63.94600868225098}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 79.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.4440637, \"EndTime\": 1644664582.5081792, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 78, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 634000.0, \"count\": 1, \"min\": 634000, \"max\": 634000}, \"Total Batches Seen\": {\"sum\": 317.0, \"count\": 1, \"min\": 317, \"max\": 317}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=124582.51619730076 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=79, batch=0 train rmse <loss>=1.7536087233723205\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=79, batch=0 train mse <loss>=3.0751435546875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=79, batch=0 train absolute_loss <loss>=1.348385986328125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.548] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 160, \"duration\": 39, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=79, train rmse <loss>=1.6037303354272288\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=79, train mse <loss>=2.5719509887695313\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=79, train absolute_loss <loss>=1.2647004699707032\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.5080824, \"EndTime\": 1644664582.5495403, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 41.17989540100098, \"count\": 1, \"min\": 41.17989540100098, \"max\": 41.17989540100098}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.508334, \"EndTime\": 1644664582.5498998, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 79, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 642000.0, \"count\": 1, \"min\": 642000, \"max\": 642000}, \"Total Batches Seen\": {\"sum\": 321.0, \"count\": 1, \"min\": 321, \"max\": 321}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 81.0, \"count\": 1, \"min\": 81, \"max\": 81}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=191772.48671200778 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=80, batch=0 train rmse <loss>=1.7525891839379388\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=80, batch=0 train mse <loss>=3.07156884765625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=80, batch=0 train absolute_loss <loss>=1.347521728515625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.591] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 162, \"duration\": 39, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=80, train rmse <loss>=1.6027395446084065\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=80, train mse <loss>=2.5687740478515626\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=80, train absolute_loss <loss>=1.2638436584472656\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.5496576, \"EndTime\": 1644664582.5920823, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 41.82028770446777, \"count\": 1, \"min\": 41.82028770446777, \"max\": 41.82028770446777}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 81.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.5502355, \"EndTime\": 1644664582.5924041, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 80, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 650000.0, \"count\": 1, \"min\": 650000, \"max\": 650000}, \"Total Batches Seen\": {\"sum\": 325.0, \"count\": 1, \"min\": 325, \"max\": 325}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 82.0, \"count\": 1, \"min\": 82, \"max\": 82}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=189017.75574583147 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=81, batch=0 train rmse <loss>=1.7515681450622682\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=81, batch=0 train mse <loss>=3.067990966796875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=81, batch=0 train absolute_loss <loss>=1.3466552734375\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.645] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 164, \"duration\": 42, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=81, train rmse <loss>=1.6017470739679356\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=81, train mse <loss>=2.5655936889648436\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=81, train absolute_loss <loss>=1.2629844360351563\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.5921812, \"EndTime\": 1644664582.6463778, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 53.62439155578613, \"count\": 1, \"min\": 53.62439155578613, \"max\": 53.62439155578613}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 82.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.592727, \"EndTime\": 1644664582.6466146, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 81, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 658000.0, \"count\": 1, \"min\": 658000, \"max\": 658000}, \"Total Batches Seen\": {\"sum\": 329.0, \"count\": 1, \"min\": 329, \"max\": 329}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 83.0, \"count\": 1, \"min\": 83, \"max\": 83}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=148124.22250572775 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=82, batch=0 train rmse <loss>=1.7505456738542986\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=82, batch=0 train mse <loss>=3.06441015625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=82, batch=0 train absolute_loss <loss>=1.3457864990234376\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.684] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 166, \"duration\": 36, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=82, train rmse <loss>=1.6007529775748799\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=82, train mse <loss>=2.562410095214844\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=82, train absolute_loss <loss>=1.2621229858398437\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.6464498, \"EndTime\": 1644664582.6848946, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 37.97292709350586, \"count\": 1, \"min\": 37.97292709350586, \"max\": 37.97292709350586}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 83.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.6469028, \"EndTime\": 1644664582.6850438, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 82, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 666000.0, \"count\": 1, \"min\": 666000, \"max\": 666000}, \"Total Batches Seen\": {\"sum\": 333.0, \"count\": 1, \"min\": 333, \"max\": 333}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 84.0, \"count\": 1, \"min\": 84, \"max\": 84}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=208990.26501821805 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=83, batch=0 train rmse <loss>=1.749521698029207\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=83, batch=0 train mse <loss>=3.060826171875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=83, batch=0 train absolute_loss <loss>=1.34491552734375\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.726] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 168, \"duration\": 39, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=83, train rmse <loss>=1.5997573096276296\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=83, train mse <loss>=2.5592234497070314\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=83, train absolute_loss <loss>=1.2612593078613281\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.6849403, \"EndTime\": 1644664582.7265074, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 41.2755012512207, \"count\": 1, \"min\": 41.2755012512207, \"max\": 41.2755012512207}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 84.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.6852078, \"EndTime\": 1644664582.7267132, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 83, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 674000.0, \"count\": 1, \"min\": 674000, \"max\": 674000}, \"Total Batches Seen\": {\"sum\": 337.0, \"count\": 1, \"min\": 337, \"max\": 337}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 85.0, \"count\": 1, \"min\": 85, \"max\": 85}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=192281.2953062055 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=84, batch=0 train rmse <loss>=1.748496424386893\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=84, batch=0 train mse <loss>=3.05723974609375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=84, batch=0 train absolute_loss <loss>=1.34404248046875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.772] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 170, \"duration\": 43, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=84, train rmse <loss>=1.5987600958224284\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=84, train mse <loss>=2.5560338439941406\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=84, train absolute_loss <loss>=1.2603934326171875\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.7265692, \"EndTime\": 1644664582.7725604, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 45.65095901489258, \"count\": 1, \"min\": 45.65095901489258, \"max\": 45.65095901489258}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 85.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.726889, \"EndTime\": 1644664582.772789, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 84, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 682000.0, \"count\": 1, \"min\": 682000, \"max\": 682000}, \"Total Batches Seen\": {\"sum\": 341.0, \"count\": 1, \"min\": 341, \"max\": 341}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 86.0, \"count\": 1, \"min\": 86, \"max\": 86}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=173849.954406035 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=85, batch=0 train rmse <loss>=1.7474697109320665\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=85, batch=0 train mse <loss>=3.053650390625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=85, batch=0 train absolute_loss <loss>=1.3431673583984376\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.813] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 172, \"duration\": 39, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=85, train rmse <loss>=1.5977614669662883\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=85, train mse <loss>=2.552841705322266\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=85, train absolute_loss <loss>=1.2595255432128907\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.772631, \"EndTime\": 1644664582.8140502, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 40.985107421875, \"count\": 1, \"min\": 40.985107421875, \"max\": 40.985107421875}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 86.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.7730436, \"EndTime\": 1644664582.8142416, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 85, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 690000.0, \"count\": 1, \"min\": 690000, \"max\": 690000}, \"Total Batches Seen\": {\"sum\": 345.0, \"count\": 1, \"min\": 345, \"max\": 345}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 87.0, \"count\": 1, \"min\": 87, \"max\": 87}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=193661.8435557505 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=86, batch=0 train rmse <loss>=1.7464417648151411\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=86, batch=0 train mse <loss>=3.050058837890625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=86, batch=0 train absolute_loss <loss>=1.3422901611328124\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.869] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 174, \"duration\": 53, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=86, train rmse <loss>=1.5967614108481667\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=86, train mse <loss>=2.549647003173828\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=86, train absolute_loss <loss>=1.2586555480957031\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.8141055, \"EndTime\": 1644664582.8703828, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 55.90224266052246, \"count\": 1, \"min\": 55.90224266052246, \"max\": 55.90224266052246}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 87.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.814455, \"EndTime\": 1644664582.870688, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 86, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 698000.0, \"count\": 1, \"min\": 698000, \"max\": 698000}, \"Total Batches Seen\": {\"sum\": 349.0, \"count\": 1, \"min\": 349, \"max\": 349}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 88.0, \"count\": 1, \"min\": 88, \"max\": 88}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=141875.60569287883 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=87, batch=0 train rmse <loss>=1.745412583858219\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=87, batch=0 train mse <loss>=3.046465087890625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=87, batch=0 train absolute_loss <loss>=1.3414110107421875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.917] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 176, \"duration\": 41, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=87, train rmse <loss>=1.5957599821571842\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=87, train mse <loss>=2.5464499206542968\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=87, train absolute_loss <loss>=1.2577835388183594\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.8704736, \"EndTime\": 1644664582.9176579, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 46.52857780456543, \"count\": 1, \"min\": 46.52857780456543, \"max\": 46.52857780456543}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 88.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.8710868, \"EndTime\": 1644664582.9178915, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 87, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 706000.0, \"count\": 1, \"min\": 706000, \"max\": 706000}, \"Total Batches Seen\": {\"sum\": 353.0, \"count\": 1, \"min\": 353, \"max\": 353}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 89.0, \"count\": 1, \"min\": 89, \"max\": 89}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=170435.21015873016 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=88, batch=0 train rmse <loss>=1.7443821658756433\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=88, batch=0 train mse <loss>=3.042869140625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=88, batch=0 train absolute_loss <loss>=1.3405299072265624\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:22.958] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 178, \"duration\": 39, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=88, train rmse <loss>=1.59475714960333\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=88, train mse <loss>=2.5432503662109376\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=88, train absolute_loss <loss>=1.256909423828125\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.917727, \"EndTime\": 1644664582.9592044, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 41.02683067321777, \"count\": 1, \"min\": 41.02683067321777, \"max\": 41.02683067321777}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #progress_metric: host=algo-1, completed 89.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.918154, \"EndTime\": 1644664582.9594228, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 88, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 714000.0, \"count\": 1, \"min\": 714000, \"max\": 714000}, \"Total Batches Seen\": {\"sum\": 357.0, \"count\": 1, \"min\": 357, \"max\": 357}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 90.0, \"count\": 1, \"min\": 90, \"max\": 90}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=193147.93580622136 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=89, batch=0 train rmse <loss>=1.7433507187355117\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=89, batch=0 train mse <loss>=3.039271728515625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:22 INFO 140146954667840] #quality_metric: host=algo-1, epoch=89, batch=0 train absolute_loss <loss>=1.3396468505859376\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:23.015] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 180, \"duration\": 54, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=89, train rmse <loss>=1.5937531020190399\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=89, train mse <loss>=2.5400489501953123\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=89, train absolute_loss <loss>=1.256033447265625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.9592688, \"EndTime\": 1644664583.016158, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 56.435346603393555, \"count\": 1, \"min\": 56.435346603393555, \"max\": 56.435346603393555}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664582.9596987, \"EndTime\": 1644664583.0163171, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 89, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 722000.0, \"count\": 1, \"min\": 722000, \"max\": 722000}, \"Total Batches Seen\": {\"sum\": 361.0, \"count\": 1, \"min\": 361, \"max\": 361}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=141059.09406200735 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=90, batch=0 train rmse <loss>=1.7423181004860306\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=90, batch=0 train mse <loss>=3.03567236328125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=90, batch=0 train absolute_loss <loss>=1.338761962890625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:23.054] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 182, \"duration\": 36, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=90, train rmse <loss>=1.5927477604651643\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=90, train mse <loss>=2.5368454284667967\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=90, train absolute_loss <loss>=1.2551554260253905\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.0162137, \"EndTime\": 1644664583.0552473, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 38.73848915100098, \"count\": 1, \"min\": 38.73848915100098, \"max\": 38.73848915100098}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #progress_metric: host=algo-1, completed 91.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.016482, \"EndTime\": 1644664583.055566, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 90, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 730000.0, \"count\": 1, \"min\": 730000, \"max\": 730000}, \"Total Batches Seen\": {\"sum\": 365.0, \"count\": 1, \"min\": 365, \"max\": 365}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 92.0, \"count\": 1, \"min\": 92, \"max\": 92}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=203975.83008109324 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=91, batch=0 train rmse <loss>=1.7412845193545339\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=91, batch=0 train mse <loss>=3.03207177734375\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=91, batch=0 train absolute_loss <loss>=1.3378751220703125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:23.097] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 184, \"duration\": 37, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=91, train rmse <loss>=1.5917411704210207\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=91, train mse <loss>=2.533639953613281\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=91, train absolute_loss <loss>=1.254275390625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.0553305, \"EndTime\": 1644664583.097524, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 41.60714149475098, \"count\": 1, \"min\": 41.60714149475098, \"max\": 41.60714149475098}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #progress_metric: host=algo-1, completed 92.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.0558934, \"EndTime\": 1644664583.0977643, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 91, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 738000.0, \"count\": 1, \"min\": 738000, \"max\": 738000}, \"Total Batches Seen\": {\"sum\": 369.0, \"count\": 1, \"min\": 369, \"max\": 369}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 93.0, \"count\": 1, \"min\": 93, \"max\": 93}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=190381.9163905406 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=92, batch=0 train rmse <loss>=1.7402499034801007\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=92, batch=0 train mse <loss>=3.0284697265625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=92, batch=0 train absolute_loss <loss>=1.336986572265625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:23.143] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 186, \"duration\": 44, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=92, train rmse <loss>=1.5907334829933064\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=92, train mse <loss>=2.530433013916016\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=92, train absolute_loss <loss>=1.2533936767578124\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.097593, \"EndTime\": 1644664583.1442018, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 46.15902900695801, \"count\": 1, \"min\": 46.15902900695801, \"max\": 46.15902900695801}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #progress_metric: host=algo-1, completed 93.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.0980206, \"EndTime\": 1644664583.1443927, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 92, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 746000.0, \"count\": 1, \"min\": 746000, \"max\": 746000}, \"Total Batches Seen\": {\"sum\": 373.0, \"count\": 1, \"min\": 373, \"max\": 373}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 94.0, \"count\": 1, \"min\": 94, \"max\": 94}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=172105.78363184998 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=93, batch=0 train rmse <loss>=1.7392143212031475\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=93, batch=0 train mse <loss>=3.024866455078125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=93, batch=0 train absolute_loss <loss>=1.3360963134765624\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:23.184] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 188, \"duration\": 38, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=93, train rmse <loss>=1.5897246577016686\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=93, train mse <loss>=2.5272244873046876\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=93, train absolute_loss <loss>=1.2525100708007812\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.144268, \"EndTime\": 1644664583.184828, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 40.18878936767578, \"count\": 1, \"min\": 40.18878936767578, \"max\": 40.18878936767578}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #progress_metric: host=algo-1, completed 94.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.1446176, \"EndTime\": 1644664583.185139, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 93, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 754000.0, \"count\": 1, \"min\": 754000, \"max\": 754000}, \"Total Batches Seen\": {\"sum\": 377.0, \"count\": 1, \"min\": 377, \"max\": 377}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 95.0, \"count\": 1, \"min\": 95, \"max\": 95}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=196607.61600075 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=94, batch=0 train rmse <loss>=1.7381777707963662\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=94, batch=0 train mse <loss>=3.021261962890625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=94, batch=0 train absolute_loss <loss>=1.3352042236328125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:23.234] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 190, \"duration\": 47, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=94, train rmse <loss>=1.5887147500054142\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=94, train mse <loss>=2.5240145568847656\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=94, train absolute_loss <loss>=1.251624725341797\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.1849318, \"EndTime\": 1644664583.235556, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 50.05598068237305, \"count\": 1, \"min\": 50.05598068237305, \"max\": 50.05598068237305}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #progress_metric: host=algo-1, completed 95.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.185473, \"EndTime\": 1644664583.235801, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 94, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 762000.0, \"count\": 1, \"min\": 762000, \"max\": 762000}, \"Total Batches Seen\": {\"sum\": 381.0, \"count\": 1, \"min\": 381, \"max\": 381}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 96.0, \"count\": 1, \"min\": 96, \"max\": 96}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=158579.31699386562 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=95, batch=0 train rmse <loss>=1.7371404613392307\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=95, batch=0 train mse <loss>=3.017656982421875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=95, batch=0 train absolute_loss <loss>=1.334310546875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:23.279] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 192, \"duration\": 41, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=95, train rmse <loss>=1.5877038443344693\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=95, train mse <loss>=2.520803497314453\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=95, train absolute_loss <loss>=1.2507376708984375\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.2356281, \"EndTime\": 1644664583.2795649, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 43.4727668762207, \"count\": 1, \"min\": 43.4727668762207, \"max\": 43.4727668762207}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #progress_metric: host=algo-1, completed 96.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.2360656, \"EndTime\": 1644664583.2797897, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 95, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 770000.0, \"count\": 1, \"min\": 770000, \"max\": 770000}, \"Total Batches Seen\": {\"sum\": 385.0, \"count\": 1, \"min\": 385, \"max\": 385}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 97.0, \"count\": 1, \"min\": 97, \"max\": 97}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=182498.90950228705 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=96, batch=0 train rmse <loss>=1.736102180532586\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=96, batch=0 train mse <loss>=3.01405078125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=96, batch=0 train absolute_loss <loss>=1.333415283203125\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:23.324] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 194, \"duration\": 43, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=96, train rmse <loss>=1.5866919291646289\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=96, train mse <loss>=2.517591278076172\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=96, train absolute_loss <loss>=1.249848907470703\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.2796354, \"EndTime\": 1644664583.325265, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 45.221805572509766, \"count\": 1, \"min\": 45.221805572509766, \"max\": 45.221805572509766}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #progress_metric: host=algo-1, completed 97.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.2800198, \"EndTime\": 1644664583.325474, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 96, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 778000.0, \"count\": 1, \"min\": 778000, \"max\": 778000}, \"Total Batches Seen\": {\"sum\": 389.0, \"count\": 1, \"min\": 389, \"max\": 389}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 98.0, \"count\": 1, \"min\": 98, \"max\": 98}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=175581.130791973 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=97, batch=0 train rmse <loss>=1.7350630673425822\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=97, batch=0 train mse <loss>=3.01044384765625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=97, batch=0 train absolute_loss <loss>=1.332518310546875\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:23.365] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 196, \"duration\": 38, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=97, train rmse <loss>=1.5856790891690933\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=97, train mse <loss>=2.514378173828125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=97, train absolute_loss <loss>=1.2489585571289064\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.3253348, \"EndTime\": 1644664583.3664927, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 40.775299072265625, \"count\": 1, \"min\": 40.775299072265625, \"max\": 40.775299072265625}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #progress_metric: host=algo-1, completed 98.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.3256917, \"EndTime\": 1644664583.366836, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 97, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 786000.0, \"count\": 1, \"min\": 786000, \"max\": 786000}, \"Total Batches Seen\": {\"sum\": 393.0, \"count\": 1, \"min\": 393, \"max\": 393}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 99.0, \"count\": 1, \"min\": 99, \"max\": 99}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=193600.3877266066 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=98, batch=0 train rmse <loss>=1.7340232610671273\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=98, batch=0 train mse <loss>=3.006836669921875\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=98, batch=0 train absolute_loss <loss>=1.3316197509765626\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:23.406] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 198, \"duration\": 37, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=98, train rmse <loss>=1.5846652936874646\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=98, train mse <loss>=2.511164093017578\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=98, train absolute_loss <loss>=1.2480665283203125\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.3666167, \"EndTime\": 1644664583.407181, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 39.845943450927734, \"count\": 1, \"min\": 39.845943450927734, \"max\": 39.845943450927734}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #progress_metric: host=algo-1, completed 99.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.3673098, \"EndTime\": 1644664583.4075305, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 98, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 794000.0, \"count\": 1, \"min\": 794000, \"max\": 794000}, \"Total Batches Seen\": {\"sum\": 397.0, \"count\": 1, \"min\": 397, \"max\": 397}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=197818.8678355402 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=99, batch=0 train rmse <loss>=1.7329826900192196\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=99, batch=0 train mse <loss>=3.00322900390625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=99, batch=0 train absolute_loss <loss>=1.3307197265625\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:23.470] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 200, \"duration\": 61, \"num_examples\": 4, \"num_bytes\": 511140}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=99, train rmse <loss>=1.5836506757775293\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=99, train mse <loss>=2.507949462890625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, epoch=99, train absolute_loss <loss>=1.2471729736328125\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, train rmse <loss>=1.5836506757775293\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, train mse <loss>=2.507949462890625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, train absolute_loss <loss>=1.2471729736328125\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.407279, \"EndTime\": 1644664583.472405, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 64.43619728088379, \"count\": 1, \"min\": 64.43619728088379, \"max\": 64.43619728088379}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.407942, \"EndTime\": 1644664583.4729302, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 99, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 802000.0, \"count\": 1, \"min\": 802000, \"max\": 802000}, \"Total Batches Seen\": {\"sum\": 401.0, \"count\": 1, \"min\": 401, \"max\": 401}, \"Max Records Seen Between Resets\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Max Batches Seen Between Resets\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Reset Count\": {\"sum\": 101.0, \"count\": 1, \"min\": 101, \"max\": 101}, \"Number of Records Since Last Reset\": {\"sum\": 8000.0, \"count\": 1, \"min\": 8000, \"max\": 8000}, \"Number of Batches Since Last Reset\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #throughput_metric: host=algo-1, train throughput=122838.91374222978 records/second\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 WARNING 140146954667840] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.472667, \"EndTime\": 1644664583.4759936, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 2.462148666381836, \"count\": 1, \"min\": 2.462148666381836, \"max\": 2.462148666381836}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] Saved checkpoint to \"/tmp/tmpjbk3te6p/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:23.481] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 5172, \"num_examples\": 1, \"num_bytes\": 127720}\u001b[0m\n",
      "\u001b[34m[2022-02-12 11:16:23.495] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 14, \"num_examples\": 1, \"num_bytes\": 127720}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.4809945, \"EndTime\": 1644664583.4957337, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2000.0, \"count\": 1, \"min\": 2000, \"max\": 2000}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 2000.0, \"count\": 1, \"min\": 2000, \"max\": 2000}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 2000.0, \"count\": 1, \"min\": 2000, \"max\": 2000}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #test_score (algo-1) : ('rmse', 1.5746361496886352)\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #test_score (algo-1) : ('mse', 2.47947900390625)\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #test_score (algo-1) : ('absolute_loss', 1.267636962890625)\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, test rmse <loss>=1.5746361496886352\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, test mse <loss>=2.47947900390625\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840] #quality_metric: host=algo-1, test absolute_loss <loss>=1.267636962890625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1644664583.4762778, \"EndTime\": 1644664583.4964626, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 20.757436752319336, \"count\": 1, \"min\": 20.757436752319336, \"max\": 20.757436752319336}, \"totaltime\": {\"sum\": 5222.03254699707, \"count\": 1, \"min\": 5222.03254699707, \"max\": 5222.03254699707}}}\u001b[0m\n",
      "\u001b[34m[02/12/2022 11:16:23 INFO 140146954667840 integration.py:636] worker closed\u001b[0m\n",
      "\n",
      "2022-02-12 11:16:40 Uploading - Uploading generated training model\n",
      "2022-02-12 11:16:40 Completed - Training job completed\n",
      "Training seconds: 92\n",
      "Billable seconds: 92\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "recommendation_estimator.fit(\n",
    "    {\n",
    "        'train': train_input,\n",
    "        'test': test_input\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "# Deploy the model\n",
    "recommendation_predictor = recommendation_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    endpoint_name=base_job_name\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
